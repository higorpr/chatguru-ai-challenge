{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\higor\\onedrive\\área de trabalho\\higor\\chatguru\\chatguru-ai-challenge\\venv\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\higor\\onedrive\\área de trabalho\\higor\\chatguru\\chatguru-ai-challenge\\venv\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\higor\\onedrive\\área de trabalho\\higor\\chatguru\\chatguru-ai-challenge\\venv\\lib\\site-packages (from nltk) (1.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\higor\\onedrive\\área de trabalho\\higor\\chatguru\\chatguru-ai-challenge\\venv\\lib\\site-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: click in c:\\users\\higor\\onedrive\\área de trabalho\\higor\\chatguru\\chatguru-ai-challenge\\venv\\lib\\site-packages (from nltk) (8.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\higor\\onedrive\\área de trabalho\\higor\\chatguru\\chatguru-ai-challenge\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.0-cp310-cp310-win_amd64.whl (9.2 MB)\n",
      "     ---------------------------------------- 9.2/9.2 MB 12.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\higor\\onedrive\\área de trabalho\\higor\\chatguru\\chatguru-ai-challenge\\venv\\lib\\site-packages (from scikit-learn) (1.25.0)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting scipy>=1.5.0\n",
      "  Downloading scipy-1.11.1-cp310-cp310-win_amd64.whl (44.0 MB)\n",
      "     --------------------------------------- 44.0/44.0 MB 31.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\higor\\onedrive\\área de trabalho\\higor\\chatguru\\chatguru-ai-challenge\\venv\\lib\\site-packages (from scikit-learn) (1.3.1)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.3.0 scipy-1.11.1 threadpoolctl-3.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words and TF-IDF\n",
    "Below, we'll look at three useful methods of vectorizing text.\n",
    "- `CountVectorizer` - Bag of Words\n",
    "- `TfidfTransformer` - TF-IDF values\n",
    "- `TfidfVectorizer` - Bag of Words AND TF-IDF values\n",
    "\n",
    "Let's first use an example from earlier and apply the text processing steps we saw in this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\higor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\higor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\higor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"The first time you see The Second Renaissance it may look boring.\",\n",
    "        \"Look at it at least twice and definitely watch part 2.\",\n",
    "        \"It will change your view of the matrix.\",\n",
    "        \"Are the human people the ones who started the war?\",\n",
    "        \"Is AI a bad thing ?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "print(stop_words)\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the skills you learned so far to create a function `tokenize` that takes in a string of text and applies the following:\n",
    "- case normalization (convert to all lowercase)\n",
    "- punctuation removal\n",
    "- tokenization, lemmatization, and stop word removal using `nltk`\n",
    "\n",
    "Feel free to refer back to previous sections to complete these steps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # normalize case and remove punctuation\n",
    "    text = re.sub(r'[^a-zA-Z0-9]',' ',text).lower()\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    tokens = [x for x in tokens if x not in stop_words]\n",
    "    \n",
    "    # Noun Lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(x) for x in tokens]\n",
    "    \n",
    "    # Verb Lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(x,pos='v') for x in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The first time you see The Second Renaissance it may look boring.', 'Look at it at least twice and definitely watch part 2.', 'It will change your view of the matrix.', 'Are the human people the ones who started the war?', 'Is AI a bad thing ?']\n",
      "['first', 'time', 'see', 'second', 'renaissance', 'may', 'look', 'bore']\n",
      "['look', 'least', 'twice', 'definitely', 'watch', 'part', '2']\n",
      "['change', 'view', 'matrix']\n",
      "['human', 'people', 'one', 'start', 'war']\n",
      "['ai', 'bad', 'thing']\n"
     ]
    }
   ],
   "source": [
    "print(corpus)\n",
    "for i,string in enumerate(corpus):\n",
    "    tokens = tokenize(string)\n",
    "    print(tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `CountVectorizer` (Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# initialize count vectorizer object\n",
    "vect = CountVectorizer(tokenizer=tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 17)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 3)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 21)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 24)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 0)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 22)\t1\n",
      "  (2, 10)\t1\n",
      "  (3, 7)\t1\n",
      "  (3, 14)\t1\n",
      "  (3, 12)\t1\n",
      "  (3, 18)\t1\n",
      "  (3, 23)\t1\n",
      "  (4, 1)\t1\n",
      "  (4, 2)\t1\n",
      "  (4, 19)\t1\n"
     ]
    }
   ],
   "source": [
    "# get counts of each token (word) in text data\n",
    "X = vect.fit_transform(corpus)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2', 'ai', 'bad', 'bore', 'change', 'definitely', 'first', 'human',\n",
       "       'least', 'look', 'matrix', 'may', 'one', 'part', 'people',\n",
       "       'renaissance', 'second', 'see', 'start', 'thing', 'time', 'twice',\n",
       "       'view', 'war', 'watch'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 0],\n",
       "       [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert sparse matrix to numpy array to view\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first': 6,\n",
       " 'time': 20,\n",
       " 'see': 17,\n",
       " 'second': 16,\n",
       " 'renaissance': 15,\n",
       " 'may': 11,\n",
       " 'look': 9,\n",
       " 'bore': 3,\n",
       " 'least': 8,\n",
       " 'twice': 21,\n",
       " 'definitely': 5,\n",
       " 'watch': 24,\n",
       " 'part': 13,\n",
       " '2': 0,\n",
       " 'change': 4,\n",
       " 'view': 22,\n",
       " 'matrix': 10,\n",
       " 'human': 7,\n",
       " 'people': 14,\n",
       " 'one': 12,\n",
       " 'start': 18,\n",
       " 'war': 23,\n",
       " 'ai': 1,\n",
       " 'bad': 2,\n",
       " 'thing': 19}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view token vocabulary and counts\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `TfidfTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# initialize tf-idf transformer object\n",
    "transformer = TfidfTransformer(smooth_idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 20)\t0.36419547310600114\n",
      "  (0, 17)\t0.36419547310600114\n",
      "  (0, 16)\t0.36419547310600114\n",
      "  (0, 15)\t0.36419547310600114\n",
      "  (0, 11)\t0.36419547310600114\n",
      "  (0, 9)\t0.2674539242255982\n",
      "  (0, 6)\t0.36419547310600114\n",
      "  (0, 3)\t0.36419547310600114\n",
      "  (1, 24)\t0.39105192975907627\n",
      "  (1, 21)\t0.39105192975907627\n",
      "  (1, 13)\t0.39105192975907627\n",
      "  (1, 9)\t0.28717647778015326\n",
      "  (1, 8)\t0.39105192975907627\n",
      "  (1, 5)\t0.39105192975907627\n",
      "  (1, 0)\t0.39105192975907627\n",
      "  (2, 22)\t0.5773502691896258\n",
      "  (2, 10)\t0.5773502691896258\n",
      "  (2, 4)\t0.5773502691896258\n",
      "  (3, 23)\t0.4472135954999579\n",
      "  (3, 18)\t0.4472135954999579\n",
      "  (3, 14)\t0.4472135954999579\n",
      "  (3, 12)\t0.4472135954999579\n",
      "  (3, 7)\t0.4472135954999579\n",
      "  (4, 19)\t0.5773502691896258\n",
      "  (4, 2)\t0.5773502691896258\n",
      "  (4, 1)\t0.5773502691896258\n"
     ]
    }
   ],
   "source": [
    "# use counts from count vectorizer results to compute tf-idf values\n",
    "tfidf = transformer.fit_transform(X)\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.36419547, 0.        ,\n",
       "        0.        , 0.36419547, 0.        , 0.        , 0.26745392,\n",
       "        0.        , 0.36419547, 0.        , 0.        , 0.        ,\n",
       "        0.36419547, 0.36419547, 0.36419547, 0.        , 0.        ,\n",
       "        0.36419547, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.39105193, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.39105193, 0.        , 0.        , 0.39105193, 0.28717648,\n",
       "        0.        , 0.        , 0.        , 0.39105193, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.39105193, 0.        , 0.        , 0.39105193],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.57735027,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.57735027, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.57735027, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.4472136 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.4472136 , 0.        , 0.4472136 ,\n",
       "        0.        , 0.        , 0.        , 0.4472136 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.4472136 , 0.        ],\n",
       "       [0.        , 0.57735027, 0.57735027, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.57735027,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert sparse matrix to numpy array to view\n",
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `TfidfVectorizer`\n",
    "`TfidfVectorizer` = `CountVectorizer` + `TfidfTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# initialize tf-idf vectorizer object\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5\n",
      "1 19\n",
      "2 16\n",
      "3 15\n",
      "4 14\n",
      "5 10\n",
      "6 8\n",
      "7 2\n",
      "8 7\n",
      "9 20\n",
      "10 4\n",
      "11 23\n",
      "12 12\n",
      "13 3\n",
      "14 21\n",
      "15 9\n",
      "16 6\n",
      "17 13\n",
      "18 11\n",
      "19 17\n",
      "20 22\n",
      "21 0\n",
      "22 1\n",
      "23 18\n"
     ]
    }
   ],
   "source": [
    "# compute bag of word counts and tf-idf values\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "for a,b in enumerate(vectorizer.vocabulary_.values()):\n",
    "    print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.36152912, 0.        , 0.        ,\n",
       "        0.36152912, 0.        , 0.        , 0.29167942, 0.        ,\n",
       "        0.36152912, 0.        , 0.        , 0.        , 0.36152912,\n",
       "        0.36152912, 0.36152912, 0.        , 0.        , 0.36152912,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.42066906,\n",
       "        0.        , 0.        , 0.42066906, 0.33939315, 0.        ,\n",
       "        0.        , 0.        , 0.42066906, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.42066906, 0.        , 0.        , 0.42066906],\n",
       "       [0.        , 0.        , 0.        , 0.57735027, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.57735027,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.57735027, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.4472136 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.4472136 , 0.        , 0.4472136 , 0.        ,\n",
       "        0.        , 0.        , 0.4472136 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.4472136 , 0.        ],\n",
       "       [0.57735027, 0.57735027, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.57735027, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert sparse matrix to numpy array to view\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
